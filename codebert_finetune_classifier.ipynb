{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3114e4b4-b5c3-468a-be6f-e7aadbad989d",
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_dir = \"./ctxt/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f072b7fc-b902-4a8b-b919-66ff4e24a22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "text_files = glob.glob(parent_dir + '*.txt')\n",
    "#print(text_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d7c68314-fc0b-4e2b-921e-5eb4cc72a6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_label_pairs = []\n",
    "labels_count = 0\n",
    "for text in text_files:\n",
    "    if labels_count == 30:\n",
    "        break\n",
    "    label = text[-25:-20]\n",
    "    if label.isdigit():\n",
    "        label = int(label)\n",
    "        #print(label)\n",
    "        with open(text, 'r', encoding='utf-8-sig') as infile:\n",
    "            program = infile.readlines()\n",
    "            for i in range(len(program)):\n",
    "                input_label_pairs.append((program[i],labels_count))\n",
    "        labels_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7e1e940e-aab6-4686-b0cb-71e3fde6e640",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/codebert-base were not used when initializing RobertaForSequenceClassification: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer, RobertaForMaskedLM, pipeline, RobertaForSequenceClassification\n",
    "model = RobertaForSequenceClassification.from_pretrained('microsoft/codebert-base',num_labels=labels_count)\n",
    "tokenizer = RobertaTokenizer.from_pretrained('microsoft/codebert-base')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "127296ad-a6b8-48bf-b021-c0986568b2fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    }
   ],
   "source": [
    "print(labels_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e2783ede-2c8f-4855-bcef-eec7720ae305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#include<iostream> using namespace std;  int main(){ \tfor (int i = 1; i <= 9; i++) \t{ \t\tfor (size_t j = 1; j <= 9; j++) \t\t{ \t\t\tint ans = i * j; \t\t\tcout << i << \"x\" << j << \"=\" << ans << endl; \t\t} \t}     return 0; }\n",
      "\n",
      "0\n",
      "#include <bits/stdc++.h> using namespace std;  #define REP(i, n) for (int i = 0; i < (n); i++)  int main() {   int a[4], b[4], s = 0, t = 0, i;    REP(i, 4) cin >> a[i];   REP(i, 4) cin >> b[i];   REP(i, 4){     s += a[i];     t += b[i];   }   cout << max(s, t) << endl; }\n",
      "\n",
      "29\n"
     ]
    }
   ],
   "source": [
    "print(input_label_pairs[0][0])\n",
    "print(input_label_pairs[0][1])\n",
    "print(input_label_pairs[-1][0])\n",
    "print(input_label_pairs[-1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d55bd24a-b8f6-459e-8551-3f00923650a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15000\n"
     ]
    }
   ],
   "source": [
    "print(len(input_label_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2c5c53d7-d423-47da-8142-ca4e9640a64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5c0f286c-4f9e-4a91-bc88-a91113c63e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class CPPDataset(Dataset):\n",
    "    def __init__(self, cpp_label_pairs, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.cpp_label_pairs = cpp_label_pairs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.cpp_label_pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        cpp_label_pair = self.cpp_label_pairs[idx]\n",
    "        input_code = self.tokenizer(cpp_label_pair[0], padding='max_length', max_length=512, truncation=True, return_tensors='pt')\n",
    "        label = cpp_label_pair[1]\n",
    "        return {\"input_ids\": input_code[\"input_ids\"], \"attention_mask\": input_code[\"attention_mask\"], \"labels\": label}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a1572768-dd48-4c94-b83b-de1d1c86c454",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4026620e-b6a9-4b9f-97b8-cc1c36f27dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, num_classes,base_model):\n",
    "        super().__init__()\n",
    "        self.codebert = base_model\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
    "        outputs = self.codebert(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        #print(outputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a857144e-77a6-4ecb-a925-9e303f578785",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7cc7bb24-573f-4ee9-b9c0-6da6d1bde4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CppTrainer:\n",
    "    def __init__(self, model, tokenizer, train_dataset, val_dataset=None):\n",
    "        torch.cuda.set_device(0)\n",
    "        torch.cuda.empty_cache()\n",
    "        self.model = Classifier(labels_count,model)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.train_dataset = train_dataset\n",
    "        self.val_dataset = val_dataset\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        print(f\"Device name: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"Total memory: {torch.cuda.get_device_properties(self.device).total_memory / 1024**2:.2f} MB\")\n",
    "        print(f\"Allocated memory: {torch.cuda.memory_allocated(self.device) / 1024**2:.2f} MB\")\n",
    "        print(f\"Free memory: {torch.cuda.memory_reserved(self.device) / 1024**2:.2f} MB\")\n",
    "\n",
    "    def train(self):\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(device)\n",
    "\n",
    "        optimizer = torch.optim.AdamW(self.model.parameters(), lr=2e-5, weight_decay=0.01)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        train_loader = DataLoader(self.train_dataset, batch_size=8, shuffle=True)\n",
    "        val_loader = DataLoader(self.val_dataset, batch_size=64) if self.val_dataset is not None else None\n",
    "\n",
    "        for epoch in range(20):\n",
    "            train_loss = 0.0\n",
    "            self.model.train()\n",
    "            c = 0\n",
    "            for batch in train_loader:\n",
    "                #print(batch)\n",
    "                c += 1;\n",
    "                if c == 2000:\n",
    "                    print(train_loss)\n",
    "                    c = 0\n",
    "                    print(\"training\")\n",
    "                input_ids = batch[\"input_ids\"].to(device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(device)\n",
    "                labels = batch[\"labels\"].to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                input_ids = input_ids.squeeze(1)\n",
    "                #print(input_ids.shape)\n",
    "                #print(labels.shape)\n",
    "                outputs = self.model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                loss = outputs.loss\n",
    "                #print(loss)\n",
    "                loss.backward()\n",
    "\n",
    "                optimizer.step()\n",
    "\n",
    "                train_loss += loss.item()\n",
    "            if val_loader is not None:\n",
    "                print(\"testing\")\n",
    "                #train_loss =10\n",
    "                self.model.eval()\n",
    "                val_loss = 0.0\n",
    "                val_correct = 0\n",
    "                with torch.no_grad():\n",
    "                    for batch in val_loader:\n",
    "                        input_ids = batch[\"input_ids\"].to(device)\n",
    "                        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "                        labels = batch[\"labels\"].to(device)\n",
    "                        input_ids = input_ids.squeeze(1)\n",
    "                        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                        _, predicted = torch.max(outputs.logits, dim=1)\n",
    "                        #print(predicted)\n",
    "                        #print(labels)\n",
    "                        val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "                    print(f\"Epoch {epoch+1}, Training Loss: {train_loss/len(self.val_dataset)}, Validation Accuracy: {val_correct/len(self.val_dataset)}, {val_correct}/{len(self.val_dataset)}\")\n",
    "\n",
    "            else:\n",
    "                print(f\"Epoch {epoch+1}, Training Loss: {train_loss/len(train_loader)}\")\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "85590e61-e790-421b-b1da-7c74a1d3abf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Split dataset into training and testing sets\n",
    "train_data, test_data = train_test_split(input_label_pairs, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create datasets using the split data\n",
    "train_dataset = CPPDataset(train_data, tokenizer)\n",
    "test_dataset = CPPDataset(test_data, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d9d7d96f-6060-44fa-8aff-d10be344b4a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device name: NVIDIA GeForce RTX 3070 Laptop GPU\n",
      "Total memory: 8191.50 MB\n",
      "Allocated memory: 0.00 MB\n",
      "Free memory: 0.00 MB\n",
      "testing\n",
      "Epoch 1, Training Loss: 0.40885557454203564, Validation Accuracy: 0.98, 2940/3000\n",
      "testing\n",
      "Epoch 2, Training Loss: 0.050088213126175105, Validation Accuracy: 0.971, 2913/3000\n",
      "testing\n",
      "Epoch 3, Training Loss: 0.031098108352472384, Validation Accuracy: 0.9823333333333333, 2947/3000\n",
      "testing\n",
      "Epoch 4, Training Loss: 0.02316103195119649, Validation Accuracy: 0.982, 2946/3000\n",
      "testing\n",
      "Epoch 5, Training Loss: 0.024299758998754743, Validation Accuracy: 0.9786666666666667, 2936/3000\n",
      "testing\n",
      "Epoch 6, Training Loss: 0.016566405191241453, Validation Accuracy: 0.982, 2946/3000\n",
      "testing\n",
      "Epoch 7, Training Loss: 0.01686328394994295, Validation Accuracy: 0.9713333333333334, 2914/3000\n",
      "testing\n",
      "Epoch 8, Training Loss: 0.015414670802109565, Validation Accuracy: 0.976, 2928/3000\n",
      "testing\n",
      "Epoch 9, Training Loss: 0.014284718309800762, Validation Accuracy: 0.9816666666666667, 2945/3000\n",
      "testing\n",
      "Epoch 10, Training Loss: 0.011550068395561539, Validation Accuracy: 0.9813333333333333, 2944/3000\n",
      "testing\n",
      "Epoch 11, Training Loss: 0.015419016797308966, Validation Accuracy: 0.9806666666666667, 2942/3000\n",
      "testing\n",
      "Epoch 12, Training Loss: 0.010195175699540414, Validation Accuracy: 0.9813333333333333, 2944/3000\n",
      "testing\n",
      "Epoch 13, Training Loss: 0.010464082945147918, Validation Accuracy: 0.9816666666666667, 2945/3000\n",
      "testing\n",
      "Epoch 14, Training Loss: 0.01028655262226918, Validation Accuracy: 0.9816666666666667, 2945/3000\n",
      "testing\n",
      "Epoch 15, Training Loss: 0.014556324286027423, Validation Accuracy: 0.9733333333333334, 2920/3000\n",
      "testing\n",
      "Epoch 16, Training Loss: 0.008628242940559479, Validation Accuracy: 0.981, 2943/3000\n",
      "testing\n",
      "Epoch 17, Training Loss: 0.009775976750524327, Validation Accuracy: 0.9813333333333333, 2944/3000\n",
      "testing\n",
      "Epoch 18, Training Loss: 0.011657318806658925, Validation Accuracy: 0.982, 2946/3000\n",
      "testing\n",
      "Epoch 19, Training Loss: 0.008060501696032589, Validation Accuracy: 0.9823333333333333, 2947/3000\n",
      "testing\n",
      "Epoch 20, Training Loss: 0.009658573413224077, Validation Accuracy: 0.979, 2937/3000\n"
     ]
    }
   ],
   "source": [
    "trainer = CppTrainer(model, tokenizer, train_dataset,test_dataset)\n",
    "trainer.train()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
